{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mini lab para procesamiento de PDFs implementando TF-IDF y luego un modelo preentrenado de textos académicos/técnicos y comparando sus resultados",
   "id": "b71fd6bcdecf2b48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Librerias requeridas",
   "id": "4bf4db70c396c48b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:55:58.612103Z",
     "start_time": "2024-11-16T18:55:58.608137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import numpy as np"
   ],
   "id": "5ecadfdf023b06c5",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Extracción de textos usando la librería PyPDF2",
   "id": "d413fce11d169a04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:57:10.944997Z",
     "start_time": "2024-11-16T18:55:58.631443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Descargar recursos de NLTK necesarios\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Cargar y dividir el PDF\n",
    "pdf = \"calculo1.pdf\"\n",
    "loader = PyPDFLoader(pdf)\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "# Extraer el contenido de texto de los documentos\n",
    "text_chunks = [sent_tokenize(doc.page_content) for doc in documents]\n",
    "text_chunks = [sentence for chunk in text_chunks for sentence in chunk]  # Aplanar la lista\n",
    "\n",
    "print(f\"Documentos desde el PDF: {len(documents)}\")\n",
    "print(f\"Fragmentos tokenizados: {len(text_chunks)}\")\n"
   ],
   "id": "48b93023a2a8430f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ivill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos desde el PDF: 848\n",
      "Fragmentos tokenizados: 22078\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocesamiento de los fragmentos\n",
    "1) Eliminar stopwords\n",
    "2) Eliminar caracteres especiales\n",
    "3) Convertir a minusculas\n",
    "4) Eliminar espacios adicionales\n",
    "5) Tokenizar y analizar con spaCy\n",
    "6) Filtrar tokens irrelevantes\n",
    "7) Lematizar palabras\n",
    "8) Reconstruir el texto limpio\n",
    "9) Agrupar fragmentos procesados en bloques más largos"
   ],
   "id": "30d60923654fb56c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:58:08.212792Z",
     "start_time": "2024-11-16T18:57:10.966134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cargar modelo de spaCy en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Stopwords dinámicas: union de spaCy y personalizadas\n",
    "spanish_stopwords = nlp.Defaults.stop_words\n",
    "custom_stopwords = {\"ejemplo\", \"puede\", \"también\"}\n",
    "stopwords = spanish_stopwords.union(custom_stopwords)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text).lower().strip()\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.lemma_ not in stopwords and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Aplicar preprocesamiento a los fragmentos\n",
    "text_chunks_cleaned = [preprocess_text(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Agrupar fragmentos en bloques más largos\n",
    "chunk_size = 3\n",
    "grouped_chunks = [\n",
    "    ' '.join(text_chunks_cleaned[i:i+chunk_size])\n",
    "    for i in range(0, len(text_chunks_cleaned), chunk_size)\n",
    "]"
   ],
   "id": "af5dea30be96544b",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Aplicar TF-IDF para convertir los fragmentos de texto en vectores dispersos que reflejan la relevancia de las palabras",
   "id": "272dce1b31ad9629"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:58:08.401107Z",
     "start_time": "2024-11-16T18:58:08.218020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_chunks)\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")"
   ],
   "id": "5ae494bc64a7031",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (22078, 13618)\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Utilizar un modelo preentrenado para generar embeddings densos que capturen el significado semántico de los fragmentos de texto",
   "id": "66df9d4e1b7be7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:04:54.755812Z",
     "start_time": "2024-11-16T18:58:08.412556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_model = SentenceTransformer('all-mpnet-base-v2') # Modelo preentrenado optimizado para textos educativos / tecnicos\n",
    "dense_embeddings = embedding_model.encode(text_chunks, convert_to_tensor=False)\n",
    "dense_embeddings_array = np.array(dense_embeddings)\n",
    "print(f\"Dense embeddings shape: {dense_embeddings_array.shape}\")"
   ],
   "id": "df803c94bf89ec24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense embeddings shape: (22078, 768)\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definir funciones para realizar búsquedas semánticas utilizando los vectores generados por TF-IDF y embeddings densos, comparando sus resultados a través de la similitud coseno",
   "id": "bd7ed160028db50c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:06:11.060058Z",
     "start_time": "2024-11-16T19:06:11.053650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def semantic_search_sparse(query, top_k=3):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    return [(text_chunks[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "def semantic_search_dense(query, top_k=3):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    dense_embeddings_tensor = torch.tensor(dense_embeddings)\n",
    "    similarities = torch.nn.functional.cosine_similarity(\n",
    "        query_embedding.unsqueeze(0), dense_embeddings_tensor, dim=-1\n",
    "    )\n",
    "    top_indices = torch.topk(similarities, top_k).indices\n",
    "    return [(text_chunks[i], similarities[i].item()) for i in top_indices]\n",
    "\n",
    "def compare_results(sparse_results, dense_results):\n",
    "    print(f\"\\n{'='*10} Comparación de Resultados Sparse vs Dense {'='*10}\")\n",
    "    for i in range(max(len(sparse_results), len(dense_results))):\n",
    "        print(f\"\\nResultado {i + 1}:\")\n",
    "        if i < len(sparse_results):\n",
    "            print(f\"  [TF-IDF] Score: {sparse_results[i][1]:.2f}\")\n",
    "            print(f\"  [TF-IDF] Fragmento: {sparse_results[i][0][:300].strip()}...\")\n",
    "        if i < len(dense_results):\n",
    "            print(f\"  [Embeddings] Score: {dense_results[i][1]:.2f}\")\n",
    "            print(f\"  [Embeddings] Fragmento: {dense_results[i][0][:300].strip()}...\")\n",
    "        print(\"-\" * 50)\n"
   ],
   "id": "b75f7a28ef7eace",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluar el laboratorio",
   "id": "72e4e6ba01ebaa93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T19:06:19.450342Z",
     "start_time": "2024-11-16T19:06:13.524650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Mini lab de Búsqueda Semántica\")\n",
    "query = input(f\"\\nIntroducir la consulta para el texto {pdf} (o escribe 'exit' para salir): \")\n",
    "sparse_results = semantic_search_sparse(query)\n",
    "dense_results = semantic_search_dense(query)\n",
    "compare_results(sparse_results, dense_results)"
   ],
   "id": "be6f2bb8c5fd5338",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laboratorio de Búsqueda Semántica\n",
      "\n",
      "========== Comparación de Resultados Sparse vs Dense ==========\n",
      "\n",
      "Resultado 1:\n",
      "  [TF-IDF] Score: 0.60\n",
      "  [TF-IDF] Fragmento: Entonces, la derivada es \n",
      "f x x 1\n",
      "2 x x 1 2....\n",
      "  [Embeddings] Score: 0.79\n",
      "  [Embeddings] Fragmento: Encuentre la primera derivada....\n",
      "--------------------------------------------------\n",
      "\n",
      "Resultado 2:\n",
      "  [TF-IDF] Score: 0.58\n",
      "  [TF-IDF] Fragmento: Por ejemplo, la tercera derivada \n",
      "es la derivada de la segunda derivada....\n",
      "  [Embeddings] Score: 0.77\n",
      "  [Embeddings] Fragmento: Compruebe su respuesta por derivación....\n",
      "--------------------------------------------------\n",
      "\n",
      "Resultado 3:\n",
      "  [TF-IDF] Score: 0.55\n",
      "  [TF-IDF] Fragmento: La segunda derivada es un ejemplo de una derivada de orden superior....\n",
      "  [Embeddings] Score: 0.76\n",
      "  [Embeddings] Fragmento: Encuentre la segunda derivada....\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 78
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
